BankNote Authentication: Neural Network From ScratchThis project implements a Backpropagation Neural Network (BPNN) using only NumPy to classify banknotes as either Authentic or Forged. It serves as a practical example of how multi-layer perceptrons learn through gradient descent and matrix calculus.üß† Project OverviewThe model is a 3-layer feedforward network:Input Layer: 4 neurons (Variance, Skewness, Kurtosis, and Entropy of the banknote image).Hidden Layer: 4 neurons using the Sigmoid activation function.Output Layer: 1 neuron (Probability: 0 for Fake, 1 for Real).üöÄ Key Machine Learning Concepts Used1. Feedforward & BackpropagationFeedforward: Data flows from input to output. We use matrix dot products to calculate the "sum" of weights and pass them through a Sigmoid function to squash values between 0 and 1.Backpropagation: The "learning" phase. We calculate the error (the difference between prediction and truth) and use the Chain Rule to update weights. We start from the output layer and work backward to the input layer.2. Sigmoid Activation & DerivativeWe use the Sigmoid function to introduce non-linearity:$$\sigma(x) = \frac{1}{1 + e^{-x}}$$The Derivative is crucial for training. It tells the model how sensitive the output is to changes in weights. If the derivative is near zero (at the flat ends of the curve), the model "learns" more slowly.3. Learning Rate ($\alpha$) with DecayA fixed learning rate can cause a model to "overshoot" the optimal solution. This script implements Learning Rate Decay:$$\alpha_{new} = \alpha_{initial} \times \frac{1}{1 + 0.01 \times \text{epoch}}$$The model takes large steps at the start to find the solution quickly and smaller, "surgical" steps as it gets closer to 100% accuracy.4. Threshold vs. AccuracyThreshold (0.01): Used during training to define "Correctness." In this code, a prediction is considered "Correct" only if it is within 0.01 of the actual value.Decision Boundary (0.5): Used during final testing. Anything above 0.5 is classified as a Real Note (1), and anything below is a Fake Note (0).üìä Evaluation: The Confusion MatrixAt the end of the training (5000 epochs), the script generates a Confusion Matrix to show precisely how the model performed on unseen data:True Positives (TP): Real notes correctly identified as real.True Negatives (TN): Fake notes correctly identified as fake.False Positives (FP): The "Type I Error"‚ÄîFake notes the model thought were real.False Negatives (FN): The "Type II Error"‚ÄîReal notes the model thought were fake.üõ†Ô∏è How to RunEnsure you have BankNote_Authentication.txt in the same directory.Install NumPy: pip install numpyRun the script: python main.pyExpected OutputYou will see the training progress every 100 epochs. A stable system should see the Test Acc (Accuracy on unseen data) steadily climb alongside the Train Acc.PlaintextEpoch 100 | Train Acc: 75.40% | Test Acc: 72.10%
...
FINAL TEST CONFUSION MATRIX
==============================
True Positives:  120
True Negatives:  145
...
FINAL TEST ACCURACY: 97.43%
üß™ Experiments to TryChange the Hidden Layer size: Change self.weight1 = np.random.rand(4, 8) and self.weight2 = np.random.rand(8, 1). Does more neurons mean better accuracy?Tweak the Decay: Change the 0.01 in the decay formula. What happens if the learning rate stays high for too long?Adjust the Threshold: See how changing the training threshold from 0.01 to 0.1 affects the speed of convergence.